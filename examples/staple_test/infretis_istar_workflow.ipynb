{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61d7de0c",
   "metadata": {},
   "source": [
    "# Interface-based State Transition Analysis and Rate calculation (iSTAR) Workflow\n",
    "\n",
    "This notebook implements the iSTAR method for calculating crossing probabilities (Pcross) from transition interface sampling (TIS) simulations using infretis weights. The iSTAR approach builds a Markov state model at the interfaces to efficiently calculate transition probabilities.\n",
    "\n",
    "## Key Features:\n",
    "- **Weight Calculation**: Uses infretis weights for proper statistical weighting of paths\n",
    "- **Transition Matrix Construction**: Builds transition matrices from interface-to-interface transition probabilities  \n",
    "- **Global Pcross Calculation**: Computes the global crossing probability using the iSTAR Markov model\n",
    "- **WHAM Integration**: Incorporates WHAM-based methods for improved statistical accuracy\n",
    "\n",
    "## Workflow Overview:\n",
    "1. Load and process infretis simulation data\n",
    "2. Calculate path weights using infretis methodology\n",
    "3. Compute transition probabilities between interfaces\n",
    "4. Construct the iSTAR transition matrix\n",
    "5. Calculate global crossing probability from the Markov model\n",
    "6. Compare with traditional WHAM methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9e8100b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Available functions:\n",
      "- get_path_weights: Calculate unbiased weights for paths\n",
      "- run_analysis: WHAM-based crossing probability analysis\n",
      "- read_toml: Read TOML configuration files\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import warnings\n",
    "\n",
    "# Inftools imports\n",
    "import sys\n",
    "sys.path.append('/mnt/0bf0c339-34bb-4500-a5fb-f3c2a863de29/DATA/APPTIS/inftools')\n",
    "\n",
    "from inftools.tistools.path_weights import get_path_weights\n",
    "from inftools.analysis.Wham_Pcross import run_analysis\n",
    "from inftools.misc.tomlreader import infretis_data_reader\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Available functions:\")\n",
    "print(\"- get_path_weights: Calculate unbiased weights for paths\")\n",
    "print(\"- run_analysis: WHAM-based crossing probability analysis\")\n",
    "print(\"- read_toml: Read TOML configuration files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5073e12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function calculate_infretis_weights updated with correct ptype handling from conv_inf_py.py!\n"
     ]
    }
   ],
   "source": [
    "def calculate_infretis_weights(data_file: str, toml_file: str, nskip: int = 0) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate infretis weights for paths, based on the path_weights.py methodology.\n",
    "    Updated to handle ptype information and derive direction from aXMYb format.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_file : str\n",
    "        Path to infretis_data.txt file\n",
    "    toml_file : str  \n",
    "        Path to infretis.toml configuration file\n",
    "    nskip : int\n",
    "        Number of initial entries to skip\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict containing path data and weights\n",
    "    \"\"\"\n",
    "    import tomli\n",
    "    import re\n",
    "    import random\n",
    "    \n",
    "    def parse_ptype_direction(ptype):\n",
    "        \"\"\"\n",
    "        Parse ptype to extract direction based on interface indices.\n",
    "        Uses the exact logic from conv_inf_py.py\n",
    "        \"\"\"\n",
    "        # Handle simple ptype formats (ensemble 0)\n",
    "        if ptype in ['RMR', 'RML', 'LMR', 'LML']:\n",
    "            return 1\n",
    "        \n",
    "        # Handle complex ptype formats with interface indices\n",
    "        # Pattern to match: digits + letters + digits\n",
    "        match = re.match(r'^(\\d+)([LR]M[LR])(\\d+)$', ptype)\n",
    "        if match:\n",
    "            try:\n",
    "                a = int(match.group(1))  # First interface index\n",
    "                b = int(match.group(3))  # Last interface index\n",
    "                \n",
    "                if a < b:\n",
    "                    return 1\n",
    "                elif a > b:\n",
    "                    return -1\n",
    "                else:  # a == b\n",
    "                    return random.choice([1, -1])\n",
    "            except ValueError:\n",
    "                # If parsing fails, default to 1\n",
    "                return 1\n",
    "        \n",
    "        # Default case\n",
    "        return 1\n",
    "\n",
    "    def extract_ptype_middle(ptype):\n",
    "        \"\"\"\n",
    "        Extract the middle part (XMX) from ptype.\n",
    "        Uses the exact logic from conv_inf_py.py\n",
    "        \"\"\"\n",
    "        # Handle simple ptype formats (ensemble 0)\n",
    "        if ptype in ['RMR', 'RML', 'LMR', 'LML']:\n",
    "            return ptype\n",
    "        \n",
    "        # Handle complex ptype formats with interface indices\n",
    "        match = re.match(r'^(\\d+)([LR]M[LR])(\\d+)$', ptype)\n",
    "        if match:\n",
    "            return match.group(2)  # Return the middle part (XMX)\n",
    "        \n",
    "        # Default case - return as is\n",
    "        return ptype\n",
    "    \n",
    "    # Load configuration\n",
    "    with open(toml_file, \"rb\") as f:\n",
    "        toml_config = tomli.load(f)\n",
    "    interfaces = toml_config[\"simulation\"][\"interfaces\"]\n",
    "    \n",
    "    # Load data\n",
    "    data = np.loadtxt(data_file, dtype=str)\n",
    "    data = data[nskip:]  # Skip initial entries\n",
    "    \n",
    "    # Check if we have ptype information (look for correct patterns)\n",
    "    has_ptype = False\n",
    "    ptype_col = None\n",
    "    \n",
    "    # Look for ptype patterns in the data - use correct patterns\n",
    "    for col in range(data.shape[1]):\n",
    "        sample_values = data[:10, col]  # Check first 10 rows\n",
    "        for val in sample_values:\n",
    "            if isinstance(val, str) and (re.match(r'\\d+[LR]M[LR]\\d+', val) or val in ['RMR', 'RML', 'LMR', 'LML']):\n",
    "                has_ptype = True\n",
    "                ptype_col = col\n",
    "                break\n",
    "        if has_ptype:\n",
    "            break\n",
    "    \n",
    "    if has_ptype:\n",
    "        print(f\"Found ptype information in column {ptype_col}\")\n",
    "        # Extract direction information from ptype\n",
    "        directions = []\n",
    "        start_interfaces = []\n",
    "        end_interfaces = []\n",
    "        \n",
    "        for i, row in enumerate(data):\n",
    "            ptype = row[ptype_col]\n",
    "            if isinstance(ptype, str):\n",
    "                # Parse using the correct logic from conv_inf_py.py\n",
    "                direction = parse_ptype_direction(ptype)\n",
    "                directions.append(direction)\n",
    "                \n",
    "                # Extract start and end interface indices\n",
    "                if ptype in ['RMR', 'RML', 'LMR', 'LML']:\n",
    "                    # Simple format - ensemble 0\n",
    "                    start_interfaces.append(0)\n",
    "                    end_interfaces.append(0)\n",
    "                else:\n",
    "                    # Complex format with interface indices\n",
    "                    match = re.match(r'^(\\d+)([LR]M[LR])(\\d+)$', ptype)\n",
    "                    if match:\n",
    "                        a = int(match.group(1))  # start interface index\n",
    "                        b = int(match.group(3))  # end interface index\n",
    "                        start_interfaces.append(a)\n",
    "                        end_interfaces.append(b)\n",
    "                    else:\n",
    "                        start_interfaces.append(0)\n",
    "                        end_interfaces.append(0)\n",
    "            else:\n",
    "                directions.append(0)  # Default for non-ptype entries\n",
    "                start_interfaces.append(0)\n",
    "                end_interfaces.append(0)\n",
    "    else:\n",
    "        print(\"No ptype information found, using standard format\")\n",
    "        directions = None\n",
    "        start_interfaces = None\n",
    "        end_interfaces = None\n",
    "    \n",
    "    # Identify non-zero paths (those with \"----\" in the zero-ensemble column)\n",
    "    # Adjust column index based on whether ptype is present\n",
    "    zero_col = 4 if not has_ptype else 5  # Assumes ptype is typically after maxop\n",
    "    if zero_col < data.shape[1]:\n",
    "        non_zero_paths = data[:, zero_col] == \"----\"\n",
    "    else:\n",
    "        # Fallback: look for \"----\" in any column after maxop\n",
    "        non_zero_paths = data[:, 3] == \"----\"\n",
    "    \n",
    "    # Replace \"----\" with \"0.0\" for numerical processing\n",
    "    data[data == \"----\"] = \"0.0\"\n",
    "    \n",
    "    # Extract path information\n",
    "    D = {}\n",
    "    D[\"pnr\"] = data[non_zero_paths, 0:1].astype(int)  # Path numbers\n",
    "    D[\"len\"] = data[non_zero_paths, 1:2].astype(int)  # Path lengths\n",
    "    D[\"maxop\"] = data[non_zero_paths, 2:3].astype(float)  # Maximum order parameter\n",
    "    \n",
    "    # Add ptype-derived information if available\n",
    "    if has_ptype:\n",
    "        D[\"ptype\"] = data[non_zero_paths, ptype_col]  # Path types\n",
    "        D[\"direction\"] = np.array([directions[i] for i in range(len(directions)) if non_zero_paths[i]])\n",
    "        D[\"start_intf\"] = np.array([start_interfaces[i] for i in range(len(start_interfaces)) if non_zero_paths[i]])\n",
    "        D[\"end_intf\"] = np.array([end_interfaces[i] for i in range(len(end_interfaces)) if non_zero_paths[i]])\n",
    "    \n",
    "    # Determine data columns for path_f and path_w\n",
    "    data_start_col = ptype_col + 1 if has_ptype else 4\n",
    "    D[\"path_f\"] = data[non_zero_paths, data_start_col : data_start_col + len(interfaces)].astype(float)  # Path occurrences\n",
    "    D[\"path_w\"] = data[non_zero_paths, data_start_col + len(interfaces) : data_start_col + 2 * len(interfaces)].astype(float)  # Path weights\n",
    "    \n",
    "    # Calculate weights w = path_f / path_w\n",
    "    w = D[\"path_f\"] / D[\"path_w\"]\n",
    "    w[np.isnan(w)] = 0\n",
    "    \n",
    "    # Normalize weights to match total number of samples\n",
    "    w = w / np.sum(w, axis=0) * np.sum(D[\"path_f\"], axis=0)\n",
    "    w[np.isnan(w)] = 0.0\n",
    "    wsum = np.sum(w, axis=0)\n",
    "    \n",
    "    # # Calculate local crossing probabilities (ploc) using WHAM\n",
    "    # ploc_wham = np.zeros(len(interfaces))\n",
    "    # ploc_wham[0] = 1.0\n",
    "    \n",
    "    # for i, intf_p1 in enumerate(interfaces[1:]):\n",
    "    #     h1 = D[\"maxop\"] >= intf_p1\n",
    "    #     nj = wsum[:i + 1]  # Number of paths crossing lambda_i for each ensemble up to i\n",
    "    #     njl = np.sum(h1 * w[:, : i + 1], axis=0)  # Number of paths crossing lambda_i+1\n",
    "    #     ploc_wham[i + 1] = np.sum(njl) / np.sum(nj / ploc_wham[: i + 1])\n",
    "    \n",
    "    # # Calculate unbiased path weights\n",
    "    # A = np.zeros_like(D[\"maxop\"])\n",
    "    # Q = 1 / np.cumsum(wsum / ploc_wham[:-1])\n",
    "    \n",
    "    # for j, pathnr in enumerate(D[\"pnr\"][:, 0]):\n",
    "    #     # Find the highest interface crossed by this path\n",
    "    #     K = min(\n",
    "    #         np.where(D[\"maxop\"][j] > interfaces)[0][-1] if np.any(D[\"maxop\"][j] > interfaces) else 0, \n",
    "    #         len(interfaces) - 2\n",
    "    #     )\n",
    "    #     A[j] = Q[K] * np.sum(w[j])\n",
    "    \n",
    "    # Store results\n",
    "    results = {\n",
    "        'interfaces': interfaces,\n",
    "        'path_data': D,\n",
    "        'weights_matrix': w,\n",
    "        # 'unbiased_weights': A,\n",
    "        # 'ploc_wham': ploc_wham,\n",
    "        # 'Q_factors': Q,\n",
    "        'has_ptype': has_ptype\n",
    "    }\n",
    "    \n",
    "    print(f\"Processed {len(D['pnr'])} non-zero paths\")\n",
    "    print(f\"Interfaces: {interfaces}\")\n",
    "    # print(f\"Local crossing probabilities (WHAM): {ploc_wham}\")\n",
    "    \n",
    "    if has_ptype:\n",
    "        print(f\"Path type information detected:\")\n",
    "        print(f\"  Forward paths (dir=1): {np.sum(D['direction'] == 1)}\")\n",
    "        print(f\"  Backward paths (dir=-1): {np.sum(D['direction'] == -1)}\")\n",
    "        print(f\"  Other paths (dir=0): {np.sum(D['direction'] == 0)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Function calculate_infretis_weights updated with correct ptype handling from conv_inf_py.py!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8cd10a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function compute_weight_matrices_weights updated with original istar_analysis.py logic!\n"
     ]
    }
   ],
   "source": [
    "def compute_weight_matrices_weights(weight_results: Dict, tr: bool = False) -> Dict:\n",
    "    \"\"\"\n",
    "    Compute 3D weight matrices from path data following original istar_analysis logic.\n",
    "    \n",
    "    This function constructs weight matrices [i,j,k] where:\n",
    "    - i: ensemble index (path ensemble)\n",
    "    - j: starting interface index\n",
    "    - k: ending interface index\n",
    "    \n",
    "    Implements the original istar_analysis.py logic for:\n",
    "    - tr (time reversal): boolean for applying time-reversal symmetry\n",
    "    - Edge case handling for specific interface transitions\n",
    "    - Proper direction mask logic and boundary conditions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    weight_results : Dict\n",
    "        Results from calculate_infretis_weights function containing:\n",
    "        - path_data (D): Dictionary with path information including path_f and path_w\n",
    "        - interfaces: List of interface positions\n",
    "        - has_ptype: Boolean indicating if ptype information is available\n",
    "    tr : bool, optional\n",
    "        If True, applies time-reversal symmetry by symmetrizing weight matrices.\n",
    "        Default is False.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict containing:\n",
    "        - weight_matrix_3d: 3D array [i,j,k] with weights for ensemble i, from interface j to k\n",
    "        - count_matrix_3d: 3D array [i,j,k] with path counts for ensemble i, from interface j to k\n",
    "        - weight_matrix_2d: 2D array [j,k] with total weights (summed over ensembles)\n",
    "        - count_matrix_2d: 2D array [j,k] with total count (summed over ensembles)\n",
    "        - ensemble_totals: 1D array with total weights per ensemble\n",
    "        - transition_summary: Dictionary with detailed transition statistics\n",
    "        - tr_applied: Boolean indicating if time-reversal symmetry was applied\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract data from weight_results\n",
    "    D = weight_results['path_data']\n",
    "    interfaces = weight_results['interfaces']\n",
    "    has_ptype = weight_results.get('has_ptype', False)\n",
    "    \n",
    "    n_interfaces = len(interfaces)\n",
    "    n_ensembles = len(interfaces)  # Number of ensembles equals number of interfaces\n",
    "    n_paths = len(D['pnr'])\n",
    "    \n",
    "    print(f\"Computing weight matrices for {n_paths} paths\")\n",
    "    print(f\"Structure: Dictionary of 2D matrices [ensemble_idx][start_interface, end_interface]\")\n",
    "    print(f\"Dimensions: {n_ensembles} ensembles x {n_interfaces} interfaces x {n_interfaces} interfaces\")\n",
    "    print(f\"Following original istar_analysis.py logic with tr={tr}\")\n",
    "    \n",
    "    # Initialize dictionaries of 2D matrices {ensemble_i: [start_interface_j, end_interface_k]}\n",
    "    weight_matrix_3d = {i: np.zeros((n_interfaces, n_interfaces)) for i in range(n_ensembles)}\n",
    "    count_matrix_3d = {i: np.zeros((n_interfaces, n_interfaces)) for i in range(n_ensembles)}\n",
    "    \n",
    "    \n",
    "    # Arrays to track totals\n",
    "    ensemble_totals = np.zeros(n_ensembles)\n",
    "    \n",
    "    if has_ptype and 'start_intf' in D and 'end_intf' in D and 'direction' in D:\n",
    "        print(\"Using ptype information with direction for istar_analysis-style computation\")\n",
    "        \n",
    "        # Process each path\n",
    "        for path_idx in range(n_paths):\n",
    "            start_intf = int(D['start_intf'][path_idx])\n",
    "            end_intf = int(D['end_intf'][path_idx])\n",
    "            direction = int(D['direction'][path_idx])  # 1 for forward, -1 for backward, 0 for other\n",
    "            \n",
    "            # Validate interface indices\n",
    "            if not (0 <= start_intf < n_interfaces and 0 <= end_intf < n_interfaces):\n",
    "                continue\n",
    "                \n",
    "            # Process each ensemble for this path (following original istar_analysis logic)\n",
    "            for i in range(n_ensembles):\n",
    "                # Calculate weight as path_f / path_w for this ensemble\n",
    "                path_f_k = D['path_f'][path_idx, i]\n",
    "                path_w_k = D['path_w'][path_idx, i]\n",
    "                \n",
    "                if path_w_k != 0 and path_f_k != 0:  # Only process non-zero entries\n",
    "                    weight = path_f_k / path_w_k\n",
    "                    \n",
    "                    # Apply original istar_analysis logic for j→k transitions\n",
    "                    j, k = start_intf, end_intf\n",
    "                    \n",
    "                    # Determine if this path should be counted in ensemble i\n",
    "                    should_count = False\n",
    "                    \n",
    "                    if j == k:\n",
    "                        # Self-transitions: Special case for i==1 (ensemble 1) and j==0\n",
    "                        if j == 0:\n",
    "                            # Original logic: count LMR paths in ensemble 1 for 0→0 transitions\n",
    "                            if 'LMR' in D['ptype'][path_idx] or ('LML' in D['ptype'][path_idx] and D['maxop'][path_idx] >= interfaces[1]):\n",
    "                                k = 1  # Adjust to next interface for ensemble 1\n",
    "                            should_count = True\n",
    "                        elif j == len(interfaces) - 1:\n",
    "                            k = len(interfaces) - 2  # Last interface self-transition\n",
    "                            should_count = True  # Original logic: count RMR paths in last ensemble\n",
    "                        else:\n",
    "                            should_count = False\n",
    "                            \n",
    "                    elif j < k:\n",
    "                        # Forward transitions (j → k where j < k)\n",
    "                        \n",
    "                        # Edge case 1: j==0 and k==1 (first interface to second)\n",
    "                        if j == 0 and k == 1:\n",
    "                            print(\"shouldnt happen first\")\n",
    "                            if i != 2:\n",
    "                                # Use direction==1 for forward paths\n",
    "                                should_count = (direction == 1)\n",
    "                            elif i == 2:\n",
    "                                # Special case: ensemble 2 uses different logic\n",
    "                                # In original: dir_mask = masks[i][\"LML\"]\n",
    "                                should_count = True  # Simplified - would need LML mask\n",
    "                                \n",
    "                        # Edge case 2: Last interface transition\n",
    "                        elif j == len(interfaces)-2 and k == len(interfaces)-1:\n",
    "                            print(\"shouldnt happen last\")\n",
    "                            # Original: dir_mask = masks[i][\"RMR\"]\n",
    "                            should_count = True  # Simplified - would need RMR mask\n",
    "                            \n",
    "                        else:\n",
    "                            # Standard forward transitions\n",
    "                            should_count = (direction == 1)\n",
    "                            \n",
    "                    else:\n",
    "                        # Backward transitions (j → k where j > k)\n",
    "                        \n",
    "                        # Edge case 1: j==1 and k==0 (second interface to first)\n",
    "                        if j == 1 and k == 0:\n",
    "                            print(\"shouldnt happen first backward\")\n",
    "                            if i != 2:\n",
    "                                # Use direction==-1 for backward paths\n",
    "                                should_count = (direction == -1)\n",
    "                            elif i == 2:\n",
    "                                # Special case: ensemble 2 uses different logic\n",
    "                                should_count = True  # Simplified - would need LML mask\n",
    "                                \n",
    "                        # Edge case 2: Last interface backward transition\n",
    "                        elif j == len(interfaces)-1 and k == len(interfaces)-2:\n",
    "                            print(\"shouldnt happen last backward\")\n",
    "                            # Original: dir_mask = masks[i][\"RMR\"]\n",
    "                            should_count = True  # Simplified - would need RMR mask\n",
    "                            \n",
    "                        else:\n",
    "                            # Standard backward transitions\n",
    "                            should_count = (direction == -1)\n",
    "                    \n",
    "                    # Count the transition if criteria are met\n",
    "                    if should_count:\n",
    "                        weight_matrix_3d[i][j, k] += weight\n",
    "                        count_matrix_3d[i][j, k] += 1\n",
    "                        ensemble_totals[i] += weight\n",
    "    \n",
    "    else:\n",
    "        print(\"No ptype information with direction available\")\n",
    "        raise ValueError(\"Path data must contain 'start_intf', 'end_intf', and 'direction' for istar_analysis-style computation.\")\n",
    "    \n",
    "    # Apply time-reversal symmetry if requested (following original istar_analysis logic)\n",
    "    tr_applied = False\n",
    "    if tr:\n",
    "        tr_applied = True\n",
    "        print(\"Applying time-reversal symmetry (tr=True)\")\n",
    "        \n",
    "        for i in range(n_ensembles):\n",
    "            # Original edge case logic for time reversal\n",
    "            if i == 2 and weight_matrix_3d[i][1, 0] == 0:\n",
    "                # In [1*] all LML paths are classified as 1 → 0 (for now).\n",
    "                # Time reversal needs to be adjusted to compensate for this\n",
    "                weight_matrix_3d[i][0, 1] *= 2\n",
    "                print(f\"  Applied tr edge case for ensemble 2: doubled weight_matrix_3d[{i}, 0, 1]\")\n",
    "                \n",
    "            elif i == len(interfaces)-1 and weight_matrix_3d[i][-2, -1] == 0:\n",
    "                weight_matrix_3d[i][-1, -2] *= 2\n",
    "                print(f\"  Applied tr edge case for last ensemble: doubled weight_matrix_3d[{i}, -1, -2]\")\n",
    "            \n",
    "            # Properly symmetrize the matrix: X[i] = (X[i] + X[i].T) / 2.0\n",
    "            weight_matrix_3d[i] = (weight_matrix_3d[i] + weight_matrix_3d[i].T) / 2.0\n",
    "            count_matrix_3d[i] = (count_matrix_3d[i] + count_matrix_3d[i].T) / 2.0\n",
    "    \n",
    "    # Calculate 2D matrices by summing over ensembles\n",
    "    weight_matrix_2d = np.zeros((n_interfaces, n_interfaces))\n",
    "    count_matrix_2d = np.zeros((n_interfaces, n_interfaces))\n",
    "\n",
    "    for i in range(n_ensembles):\n",
    "        weight_matrix_2d += weight_matrix_3d[i]\n",
    "        count_matrix_2d += count_matrix_3d[i]\n",
    "    \n",
    "    # Create transition summary\n",
    "    total_weight = sum(np.sum(weight_matrix_3d[i]) for i in range(n_ensembles))\n",
    "    total_transitions = sum(np.sum(count_matrix_3d[i]) for i in range(n_ensembles))\n",
    "    \n",
    "    # Analyze transition types across all ensembles\n",
    "    forward_transitions = 0\n",
    "    backward_transitions = 0\n",
    "    self_transitions = 0\n",
    "    forward_weight = 0\n",
    "    backward_weight = 0\n",
    "    self_weight = 0\n",
    "    \n",
    "    for i in range(n_ensembles):\n",
    "        for j in range(n_interfaces):\n",
    "            for k in range(n_interfaces):\n",
    "                weight_ijk = weight_matrix_3d[i][j, k]\n",
    "                count_ijk = count_matrix_3d[i][j, k]\n",
    "                \n",
    "                if count_ijk > 0:\n",
    "                    if j < k:  # Forward transition\n",
    "                        forward_transitions += count_ijk\n",
    "                        forward_weight += weight_ijk\n",
    "                    elif j > k:  # Backward transition\n",
    "                        backward_transitions += count_ijk\n",
    "                        backward_weight += weight_ijk\n",
    "                    else:  # Self transition\n",
    "                        self_transitions += count_ijk\n",
    "                        self_weight += weight_ijk\n",
    "    \n",
    "    transition_summary = {\n",
    "        'total_weight': total_weight,\n",
    "        'total_transitions': total_transitions,\n",
    "        'forward_transitions': forward_transitions,\n",
    "        'backward_transitions': backward_transitions,\n",
    "        'self_transitions': self_transitions,\n",
    "        'forward_weight': forward_weight,\n",
    "        'backward_weight': backward_weight,\n",
    "        'self_weight': self_weight,\n",
    "        'forward_weight_fraction': forward_weight / total_weight if total_weight > 0 else 0,\n",
    "        'backward_weight_fraction': backward_weight / total_weight if total_weight > 0 else 0,\n",
    "        'self_weight_fraction': self_weight / total_weight if total_weight > 0 else 0\n",
    "    }\n",
    "    \n",
    "    # Print detailed results following original istar_analysis style\n",
    "    print(f\"\\n=== 3D WEIGHT MATRICES RESULTS (istar_analysis style) ===\")\n",
    "    print(f\"3D Matrix dimensions: {n_ensembles} x {n_interfaces} x {n_interfaces}\")\n",
    "    print(f\"Total weight processed: {total_weight:.6f}\")\n",
    "    print(f\"Total transitions: {total_transitions}\")\n",
    "    print(f\"Non-zero 3D matrix elements: {sum(np.count_nonzero(weight_matrix_3d[i]) for i in range(n_ensembles))}\")\n",
    "    print(f\"Time-reversal symmetry applied: {tr_applied}\")\n",
    "    \n",
    "    # Print ensemble weights (like original \"Sum weights ensemble i\")\n",
    "    print(f\"\\nEnsemble weight totals:\")\n",
    "    for i in range(n_ensembles):\n",
    "        ensemble_sum = np.sum(weight_matrix_3d[i])\n",
    "        print(f\"  Sum weights ensemble {i}: {ensemble_sum:.4f}\")\n",
    "    \n",
    "    print(f\"\\n2D Weight Matrix [start_interface, end_interface] (summed over ensembles):\")\n",
    "    print(\"Rows = start interface, Columns = end interface\")\n",
    "    for j in range(n_interfaces):\n",
    "        row_str = f\"Interface {j}: \"\n",
    "        for k in range(n_interfaces):\n",
    "            row_str += f\"{weight_matrix_2d[j, k]:8.4f} \"\n",
    "        print(row_str)\n",
    "    \n",
    "    print(f\"\\nTransition Analysis:\")\n",
    "    print(f\"Forward transitions (j<k):  {forward_transitions:4f} paths, {forward_weight:8.4f} weight ({transition_summary['forward_weight_fraction']:.1%})\")\n",
    "    print(f\"Backward transitions (j>k): {backward_transitions:4f} paths, {backward_weight:8.4f} weight ({transition_summary['backward_weight_fraction']:.1%})\")\n",
    "    print(f\"Self transitions (j=k):     {self_transitions:4f} paths, {self_weight:8.4f} weight ({transition_summary['self_weight_fraction']:.1%})\")\n",
    "\n",
    "    # Show some 3D matrix details for non-zero entries\n",
    "    print(f\"\\nNon-zero 3D matrix entries (first 10):\")\n",
    "    count = 0\n",
    "    for i in range(n_ensembles):\n",
    "        for j in range(n_interfaces):\n",
    "            for k in range(n_interfaces):\n",
    "                if weight_matrix_3d[i][j, k] > 0 and count < 10:\n",
    "                    print(f\"  weights[{i},{j},{k}] = {weight_matrix_3d[i][j, k]:.6f} (count: {count_matrix_3d[i][j, k]:.1f})\")\n",
    "                    count += 1\n",
    "                if count >= 10:\n",
    "                    break\n",
    "            if count >= 10:\n",
    "                break\n",
    "        if count >= 10:\n",
    "            break\n",
    "    \n",
    "    # Store and return results\n",
    "    results = {\n",
    "        'weight_matrix_3d': weight_matrix_3d,\n",
    "        'count_matrix_3d': count_matrix_3d,\n",
    "        'weight_matrix_2d': weight_matrix_2d,\n",
    "        'count_matrix_2d': count_matrix_2d,\n",
    "        'ensemble_totals': ensemble_totals,\n",
    "        'transition_summary': transition_summary,\n",
    "        'tr_applied': tr_applied,\n",
    "        'interfaces': interfaces,\n",
    "        'n_interfaces': n_interfaces,\n",
    "        'n_ensembles': n_ensembles,\n",
    "        'total_paths_processed': n_paths\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Function compute_weight_matrices_weights updated with original istar_analysis.py logic!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ed595f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iSTAR matrix construction functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def construct_istar_transition_matrix(P: np.ndarray, N: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Construct the iSTAR transition matrix M from interface-to-interface transition probabilities.\n",
    "    \n",
    "    This implements the iSTAR model structure where states are defined at each interface\n",
    "    with special handling for boundary states.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    P : np.ndarray\n",
    "        Array of probabilities for paths between interfaces. P[i,j] represents the \n",
    "        probability of a path transitioning from interface i to interface j.\n",
    "    N : int\n",
    "        Number of interfaces in the system.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Transition matrix M for the iSTAR model with dimensions (2*N, 2*N).\n",
    "    \"\"\"\n",
    "    NS = 2 * N  # Dimension of the Markov state model\n",
    "    \n",
    "    # Validate input dimensions\n",
    "    if P.shape[0] != N or P.shape[1] != N:\n",
    "        raise ValueError(f\"P matrix dimensions {P.shape} don't match N={N}\")\n",
    "    \n",
    "    # Construct transition matrix\n",
    "    M = np.zeros((NS, NS))\n",
    "    \n",
    "    # States [0-] and [0*+-] - special boundary handling\n",
    "    M[0, 2] = 1            # Transition from state 0 to state 2\n",
    "    M[2, 0] = P[0, 0]      # Transition from state 2 to state 0\n",
    "    M[2, N+1:] = P[0, 1:]  # Transitions from state 2 to states N+1 and beyond\n",
    "    M[1, 0] = 1            # Transition from state 1 to state 0\n",
    "    M[-1, 0] = 1           # Transition from last state to state 0\n",
    "    \n",
    "    # Transitions from intermediate states to boundary\n",
    "    if N > 1:\n",
    "        M[N+1:-1, 1] = P[1:-1, 0]  # Transitions to state 1\n",
    "    \n",
    "    # Set up transitions for other interfaces\n",
    "    for i in range(1, N-1):\n",
    "        # Forward transitions\n",
    "        if i < N-1:\n",
    "            M[N+i, N+i+1:] = P[i, i+1:]\n",
    "        # Backward transitions\n",
    "        if i > 0:\n",
    "            M[N+i, N+1:N+i] = P[i, 1:i]\n",
    "    \n",
    "    # Handle the last interface (special case)\n",
    "    if N > 1:\n",
    "        M[N, -1] = P[N-1, N-1] if N-1 < P.shape[1] else 0  # Self-transition or to absorbing state\n",
    "    \n",
    "    # Ensure matrix is properly stochastic (rows sum to 1)\n",
    "    for i in range(NS):\n",
    "        row_sum = np.sum(M[i, :])\n",
    "        if row_sum == 0:\n",
    "            M[i, i] = 1  # Set diagonal entry to 1 for rows that sum to zero\n",
    "        elif row_sum > 0 and not np.isclose(row_sum, 1.0):\n",
    "            M[i, :] /= row_sum  # Normalize to ensure stochastic matrix\n",
    "    \n",
    "    print(f\"Constructed iSTAR transition matrix with shape {M.shape}\")\n",
    "    print(f\"Matrix properties:\")\n",
    "    print(f\"- Row sums: {[np.sum(M[i, :]) for i in range(min(5, NS))]}\")\n",
    "    print(f\"- Non-zero entries: {np.count_nonzero(M)}/{NS*NS}\")\n",
    "    \n",
    "    return M\n",
    "\n",
    "def global_pcross_istar(M: np.ndarray, doprint: bool = False) -> Tuple:\n",
    "    \"\"\"\n",
    "    Calculate global crossing probabilities from a transition matrix using the iSTAR approach.\n",
    "    \n",
    "    This function computes the probability to arrive at state -1 before reaching state 0,\n",
    "    given that we start at state 0 and leave it.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    M : np.ndarray\n",
    "        Transition matrix representing the Markov state model\n",
    "    doprint : bool\n",
    "        If True, prints detailed calculation information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (z1, z2, y1, y2) - solution vectors from the iSTAR calculation\n",
    "    \"\"\"\n",
    "    NS = len(M)\n",
    "    if NS <= 2:\n",
    "        print(\"Warning: Matrix too small for meaningful iSTAR analysis\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Extract submatrices from the transition matrix\n",
    "    Mp = M[2:-1, 2:-1]  # Transition probabilities between intermediate states\n",
    "    a = np.identity(NS-3) - Mp  # I - Mp, used for solving linear system\n",
    "    \n",
    "    # Extract other submatrices from the transition matrix\n",
    "    D = M[2:-1, np.array([0, -1])]  # Transitions from intermediate states to boundary states\n",
    "    E = M[np.array([0, -1]), 2:-1]  # Transitions from boundary states to intermediate states\n",
    "    M11 = M[np.array([0, -1]), np.array([0, -1])]  # Transitions between boundary states\n",
    "    \n",
    "    # Compute Z vector - solve the linear system\n",
    "    z1 = np.array([[0], [1]])  # Initial condition\n",
    "    try:\n",
    "        z2 = np.linalg.solve(a, np.dot(D, z1))  # Solve (I-Mp)z2 = D·z1\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"Warning: Singular matrix encountered in iSTAR calculation\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Compute H vector - final probability distribution\n",
    "    y1 = np.dot(M11, z1) + np.dot(E, z2)  # Transitions to boundary states\n",
    "    y2 = np.dot(D, z1) + np.dot(Mp, z2)   # Transitions to intermediate states\n",
    "    \n",
    "    if doprint:\n",
    "        print(\"iSTAR calculation details:\")\n",
    "        print(f\"Intermediate states matrix Mp shape: {Mp.shape}\")\n",
    "        print(f\"Boundary transition matrix D shape: {D.shape}\")\n",
    "        print(f\"z1 (boundary conditions): {z1.flatten()}\")\n",
    "        print(f\"z2 (intermediate solutions): {z2.flatten()}\")\n",
    "        print(f\"y1 (boundary probabilities): {y1.flatten()}\")\n",
    "        print(f\"Global crossing probability: {y1[1, 0] if y1.shape[0] > 1 else 'N/A'}\")\n",
    "    \n",
    "    return z1, z2, y1, y2\n",
    "\n",
    "print(\"iSTAR matrix construction functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1fec66ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binless crossing probability and plotting functions defined successfully!\n"
     ]
    }
   ],
   "source": [
    "def calculate_binless_pcross(weight_results: Dict) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Calculate binless crossing probability from infretis weights.\n",
    "    \n",
    "    This is based on the method in path_weights.py and provides a direct\n",
    "    calculation of the crossing probability curve.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    weight_results : Dict\n",
    "        Results from calculate_infretis_weights function\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (order_params, pcross_values) - arrays for plotting/analysis\n",
    "    \"\"\"\n",
    "    D = weight_results['path_data']\n",
    "    A = weight_results['unbiased_weights']\n",
    "    interfaces = weight_results['interfaces']\n",
    "    \n",
    "    # Sort paths by maximum order parameter\n",
    "    idx = np.argsort(D[\"maxop\"].flatten())\n",
    "    maxop_sorted = D[\"maxop\"][idx].flatten()\n",
    "    weight_sorted = A[idx].flatten()\n",
    "    sumw = np.sum(weight_sorted)\n",
    "    \n",
    "    # Calculate crossing probability\n",
    "    res_y = [1.0]\n",
    "    res_x = [interfaces[0]]\n",
    "    \n",
    "    for i, moi in enumerate(maxop_sorted):\n",
    "        # Skip duplicate order parameter values\n",
    "        if res_x[-1] == moi:\n",
    "            continue\n",
    "        # Crossing probability = sum of weights of paths that cross this point / total weight\n",
    "        res_y.append(np.sum(weight_sorted[i:]) / sumw)\n",
    "        res_x.append(moi)\n",
    "    \n",
    "    res_x = np.array(res_x)\n",
    "    res_y = np.array(res_y)\n",
    "    \n",
    "    print(f\"Calculated binless crossing probability with {len(res_x)} points\")\n",
    "    print(f\"Order parameter range: {res_x.min():.3f} to {res_x.max():.3f}\")\n",
    "    print(f\"Crossing probability range: {res_y.min():.6f} to {res_y.max():.6f}\")\n",
    "    \n",
    "    return res_x, res_y\n",
    "\n",
    "def plot_pcross_comparison(results_dict: Dict, title: str = \"Crossing Probability Comparison\"):\n",
    "    \"\"\"\n",
    "    Plot comparison of different crossing probability calculations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    results_dict : Dict\n",
    "        Dictionary containing different pcross calculations with keys:\n",
    "        - 'binless': (x, y) tuple from binless calculation\n",
    "        - 'istar': global crossing probability from iSTAR\n",
    "        - 'interfaces': interface positions\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot binless crossing probability\n",
    "    if 'binless' in results_dict:\n",
    "        x_binless, y_binless = results_dict['binless']\n",
    "        plt.semilogy(x_binless, y_binless, 'b-', linewidth=2, label='Binless (infretis weights)')\n",
    "    \n",
    "    # Plot interfaces\n",
    "    if 'interfaces' in results_dict:\n",
    "        interfaces = results_dict['interfaces']\n",
    "        for i, intf in enumerate(interfaces):\n",
    "            plt.axvline(intf, color='black', linestyle='--', alpha=0.7, linewidth=1)\n",
    "            plt.text(intf, 0.5, f'λ_{i}', rotation=90, verticalalignment='center')\n",
    "    \n",
    "    # Add iSTAR global crossing probability if available\n",
    "    if 'istar_global' in results_dict:\n",
    "        istar_pcross = results_dict['istar_global']\n",
    "        plt.axhline(istar_pcross, color='red', linestyle='-', linewidth=2, \n",
    "                   label=f'iSTAR Global P_cross = {istar_pcross:.6f}')\n",
    "    \n",
    "    plt.xlabel('Order Parameter (λ)')\n",
    "    plt.ylabel('Crossing Probability P(λ)')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(1e-6, 1.1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Binless crossing probability and plotting functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b81c9b",
   "metadata": {},
   "source": [
    "## Data Loading and Configuration\n",
    "\n",
    "In this section, we'll load the infretis simulation data and configure the analysis parameters. The example uses the test data from the double-well system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1c9a4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data generation functions updated with correct ptype format!\n"
     ]
    }
   ],
   "source": [
    "def create_test_data_with_ptype(filename: str, n_paths: int = 50, n_interfaces: int = 4):\n",
    "    \"\"\"\n",
    "    Create test infretis_data.txt with ptype information for demonstration.\n",
    "    \n",
    "    This generates synthetic data with the correct aXMXb ptype format where:\n",
    "    - a = start interface index\n",
    "    - b = end interface index  \n",
    "    - X can be L or R (representing left/right movement)\n",
    "    - direction = 1 if a < b, -1 if a > b, random if a == b\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filename : str\n",
    "        Output filename for the test data\n",
    "    n_paths : int\n",
    "        Number of paths to generate\n",
    "    n_interfaces : int\n",
    "        Number of interfaces\n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    interfaces = [-1.0, -0.5, 0.0, 0.5, 1.0][:n_interfaces+1]  # Example interface positions\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        # Write header\n",
    "        f.write(\"# Test data with ptype information\\n\")\n",
    "        f.write(\"# path_nr\\tlength\\tmaxop\\tptype\\t\")\n",
    "        f.write(\"\\t\".join([f\"ens{i:03d}\" for i in range(n_interfaces)]))\n",
    "        f.write(\"\\t\")\n",
    "        f.write(\"\\t\".join([f\"wgt{i:03d}\" for i in range(n_interfaces)]))\n",
    "        f.write(\"\\n\")\n",
    "        \n",
    "        # Generate synthetic paths\n",
    "        for path_nr in range(n_paths):\n",
    "            # Random path properties\n",
    "            length = random.randint(10, 100)\n",
    "            \n",
    "            # Random start and end interfaces\n",
    "            start_intf = random.randint(0, n_interfaces-1)\n",
    "            end_intf = random.randint(0, n_interfaces-1)\n",
    "            \n",
    "            # Create ptype in correct aXMXb format from conv_inf_py.py\n",
    "            if start_intf == 0 and end_intf == 0:\n",
    "                # Simple format for ensemble 0\n",
    "                ptype = random.choice(['RMR', 'RML', 'LMR', 'LML'])\n",
    "            else:\n",
    "                # Complex format with interface indices\n",
    "                # Choose random middle part\n",
    "                middle_parts = ['LMR', 'RML', 'LML', 'RMR']\n",
    "                middle = random.choice(middle_parts)\n",
    "                ptype = f\"{start_intf}{middle}{end_intf}\"\n",
    "            \n",
    "            # Maximum order parameter should be consistent with the path\n",
    "            if end_intf >= start_intf:  # Forward path\n",
    "                maxop = random.uniform(interfaces[start_intf], interfaces[end_intf+1] if end_intf < len(interfaces)-1 else 1.5)\n",
    "            else:  # Backward path\n",
    "                maxop = random.uniform(interfaces[end_intf], interfaces[start_intf+1] if start_intf < len(interfaces)-1 else 1.5)\n",
    "            \n",
    "            # Generate ensemble data (path_f and path_w)\n",
    "            path_f = []\n",
    "            path_w = []\n",
    "            \n",
    "            for i in range(n_interfaces):\n",
    "                if i == start_intf:\n",
    "                    # Path contributes to starting ensemble\n",
    "                    f_val = random.uniform(0.5, 2.0)\n",
    "                    w_val = random.uniform(0.5, 2.0)\n",
    "                    path_f.append(f_val)\n",
    "                    path_w.append(w_val)\n",
    "                else:\n",
    "                    # No contribution to other ensembles\n",
    "                    path_f.append(\"----\")\n",
    "                    path_w.append(\"----\")\n",
    "            \n",
    "            # Write path data\n",
    "            f.write(f\"{path_nr}\\t{length}\\t{maxop:.5f}\\t{ptype}\\t\")\n",
    "            f.write(\"\\t\".join([str(x) for x in path_f]))\n",
    "            f.write(\"\\t\")\n",
    "            f.write(\"\\t\".join([str(x) for x in path_w]))\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    print(f\"Created test data file: {filename}\")\n",
    "    print(f\"Generated {n_paths} paths with {n_interfaces} interfaces\")\n",
    "    print(f\"File includes ptype information in correct aXMXb format\")\n",
    "\n",
    "# Test if we want to demonstrate with synthetic data\n",
    "def test_ptype_format():\n",
    "    \"\"\"Test the ptype format parsing with synthetic data\"\"\"\n",
    "    test_file = \"test_infretis_data_with_ptype.txt\"\n",
    "    create_test_data_with_ptype(test_file, n_paths=20, n_interfaces=3)\n",
    "    \n",
    "    # Show first few lines\n",
    "    with open(test_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    print(\"\\nFirst few lines of generated test data:\")\n",
    "    for i, line in enumerate(lines[:8]):\n",
    "        print(f\"{i+1}: {line.strip()}\")\n",
    "    \n",
    "    return test_file\n",
    "\n",
    "print(\"Test data generation functions updated with correct ptype format!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c81eed51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "Data directory: /mnt/0bf0c339-34bb-4500-a5fb-f3c2a863de29/DATA/APPTIS/inftools/test/test_staple\n",
      "TOML file: /mnt/0bf0c339-34bb-4500-a5fb-f3c2a863de29/DATA/APPTIS/inftools/test/test_staple/infretis.toml\n",
      "Data file: /mnt/0bf0c339-34bb-4500-a5fb-f3c2a863de29/DATA/APPTIS/inftools/test/test_staple/infretis_data.txt\n",
      "✓ TOML file found\n",
      "✓ Data file found\n",
      "  - Total lines: 15126\n",
      "  - First few lines:\n",
      "    1: # ==========================================================================\n",
      "    2: # \txxx\tlen\tmax OP\t\t000\t001\t002\t003\t004\n",
      "    3: # ==========================================================================\n",
      "    4: 3\t 1170\t 0.30010\t-0.10011\t0LMR4\t----\t----\t----\t----\t----\t----\t----\t----\t----\t----\n",
      "    5: 0\t 3021\t-0.09986\t-0.36087\tRMR\t1.0\t----\t----\t----\t----\t1.0\t----\t----\t----\t----\n",
      "\\nUsing standard data format (no ptype information)\n",
      "Set USE_TEST_PTYPE_DATA = True to demonstrate ptype functionality\n"
     ]
    }
   ],
   "source": [
    "# Configuration for the analysis\n",
    "DATA_DIR = \"/mnt/0bf0c339-34bb-4500-a5fb-f3c2a863de29/DATA/APPTIS/inftools/test/test_staple\"\n",
    "TOML_FILE = f\"{DATA_DIR}/infretis.toml\"\n",
    "DATA_FILE = f\"{DATA_DIR}/infretis_data.txt\"\n",
    "\n",
    "# Analysis parameters\n",
    "NSKIP = 0  # Number of initial entries to skip\n",
    "VERBOSE = True\n",
    "USE_TEST_PTYPE_DATA = False  # Set to True to demonstrate ptype functionality\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"TOML file: {TOML_FILE}\")\n",
    "print(f\"Data file: {DATA_FILE}\")\n",
    "\n",
    "# Check if files exist\n",
    "import os\n",
    "if os.path.exists(TOML_FILE):\n",
    "    print(\"✓ TOML file found\")\n",
    "else:\n",
    "    print(\"✗ TOML file not found\")\n",
    "    \n",
    "if os.path.exists(DATA_FILE):\n",
    "    print(\"✓ Data file found\")\n",
    "    # Show basic info about the data file\n",
    "    with open(DATA_FILE, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    print(f\"  - Total lines: {len(lines)}\")\n",
    "    print(f\"  - First few lines:\")\n",
    "    for i, line in enumerate(lines[:5]):\n",
    "        print(f\"    {i+1}: {line.strip()}\")\n",
    "else:\n",
    "    print(\"✗ Data file not found\")\n",
    "\n",
    "# Option to demonstrate ptype functionality\n",
    "if USE_TEST_PTYPE_DATA:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"DEMONSTRATING PTYPE FUNCTIONALITY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create test data with ptype information\n",
    "    test_data_file = test_ptype_format()\n",
    "    \n",
    "    # Create a simple TOML config for the test\n",
    "    test_toml_content = '''\n",
    "    [simulation]\n",
    "    interfaces = [-1.0, -0.5, 0.0, 0.5]\n",
    "    '''\n",
    "    \n",
    "    test_toml_file = \"test_infretis.toml\"\n",
    "    with open(test_toml_file, 'w') as f:\n",
    "        f.write(test_toml_content)\n",
    "    \n",
    "    print(f\"\\\\nSwitching to test data for ptype demonstration:\")\n",
    "    print(f\"Test TOML file: {test_toml_file}\")\n",
    "    print(f\"Test data file: {test_data_file}\")\n",
    "    \n",
    "    # Override the configuration\n",
    "    TOML_FILE = test_toml_file\n",
    "    DATA_FILE = test_data_file\n",
    "    \n",
    "    print(\"\\\\n✓ Test configuration ready for ptype demonstration\")\n",
    "else:\n",
    "    print(f\"\\\\nUsing standard data format (no ptype information)\")\n",
    "    print(f\"Set USE_TEST_PTYPE_DATA = True to demonstrate ptype functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1444f7ba",
   "metadata": {},
   "source": [
    "## Step 1: Calculate Infretis Weights\n",
    "\n",
    "First, we calculate the unbiased weights for all paths using the infretis methodology. This is the key improvement over the old workflow - using proper infretis weights instead of simple path counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f27d463e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA FORMAT ANALYSIS ===\n",
      "Data shape: (10, 15)\n",
      "Number of columns: 15\n",
      "Found ptype pattern '0LMR4' in column 4, row 0\n",
      "  -> Start interface: 0, End interface: 4, Middle: LMR, Direction: 1\n",
      "Found ptype pattern 'RMR' in column 4, row 1\n",
      "  -> Simple ptype format, Direction: 1\n",
      "Found ptype pattern '0LMR0' in column 4, row 2\n",
      "  -> Start interface: 0, End interface: 0, Middle: LMR, Direction: 1\n",
      "Found ptype pattern '0LMR4' in column 4, row 3\n",
      "  -> Start interface: 0, End interface: 4, Middle: LMR, Direction: 1\n",
      "Found ptype pattern '4RML0' in column 4, row 4\n",
      "  -> Start interface: 4, End interface: 0, Middle: RML, Direction: -1\n",
      "Found ptype pattern '0LML0' in column 4, row 5\n",
      "  -> Start interface: 0, End interface: 0, Middle: LML, Direction: 1\n",
      "Found ptype pattern 'RMR' in column 4, row 6\n",
      "  -> Simple ptype format, Direction: 1\n",
      "Found ptype pattern '0LMR4' in column 4, row 7\n",
      "  -> Start interface: 0, End interface: 4, Middle: LMR, Direction: 1\n",
      "Found ptype pattern 'RMR' in column 4, row 8\n",
      "  -> Simple ptype format, Direction: 1\n",
      "Found ptype pattern 'RMR' in column 4, row 9\n",
      "  -> Simple ptype format, Direction: 1\n",
      "\n",
      "Ptype examples found: ['0LMR4', '0LMR0', '4RML0', '0LML0', 'RMR']\n",
      "\n",
      "Data preview (first 3 rows):\n",
      "Row 0: ['3' '1170' '0.30010' '-0.10011' '0LMR4' '----' '----' '----' '----'\n",
      " '----' '----' '----' '----' '----' '----']\n",
      "Row 1: ['0' '3021' '-0.09986' '-0.36087' 'RMR' '1.0' '----' '----' '----' '----'\n",
      " '1.0' '----' '----' '----' '----']\n",
      "Row 2: ['1' '1816' '0.00423' '-0.10015' '0LMR0' '----' '0.5' '0.5' '----' '----'\n",
      " '----' '1.0' '1.0' '----' '----']\n",
      "\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_184690/380331464.py:6: UserWarning: Input line 1 contained no data and will not be counted towards `max_rows=10`.  This differs from the behaviour in NumPy <=1.22 which counted lines rather than rows.  If desired, the previous behaviour can be achieved by using `itertools.islice`.\n",
      "Please see the 1.23 release notes for an example on how to do this.  If you wish to ignore this warning, use `warnings.filterwarnings`.  This warning is expected to be removed in the future and is given only once per `loadtxt` call.\n",
      "  data_sample = np.loadtxt(DATA_FILE, dtype=str, max_rows=10)\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate format detection and ptype parsing\n",
    "print(\"=== DATA FORMAT ANALYSIS ===\")\n",
    "\n",
    "# Load and analyze the data format\n",
    "try:\n",
    "    data_sample = np.loadtxt(DATA_FILE, dtype=str, max_rows=10)\n",
    "    print(f\"Data shape: {data_sample.shape}\")\n",
    "    print(f\"Number of columns: {data_sample.shape[1]}\")\n",
    "    \n",
    "    # Check for ptype patterns - correct pattern from conv_inf_py.py\n",
    "    import re\n",
    "    ptype_found = False\n",
    "    ptype_examples = []\n",
    "    \n",
    "    for col in range(data_sample.shape[1]):\n",
    "        for row in range(data_sample.shape[0]):\n",
    "            val = data_sample[row, col]\n",
    "            # Look for patterns like \"0LMR4\", \"10RML12\", or simple \"RMR\", \"LML\" etc.\n",
    "            if isinstance(val, str) and (re.match(r'\\d+[LR]M[LR]\\d+', val) or val in ['RMR', 'RML', 'LMR', 'LML']):\n",
    "                ptype_found = True\n",
    "                ptype_examples.append(val)\n",
    "                print(f\"Found ptype pattern '{val}' in column {col}, row {row}\")\n",
    "                \n",
    "                # Parse and show direction using the correct logic from conv_inf_py.py\n",
    "                if val in ['RMR', 'RML', 'LMR', 'LML']:\n",
    "                    # Simple format (ensemble 0)\n",
    "                    direction = 1\n",
    "                    print(f\"  -> Simple ptype format, Direction: {direction}\")\n",
    "                else:\n",
    "                    # Complex format with interface indices\n",
    "                    match = re.match(r'^(\\d+)([LR]M[LR])(\\d+)$', val)\n",
    "                    if match:\n",
    "                        a = int(match.group(1))  # First interface index\n",
    "                        b = int(match.group(3))  # Last interface index\n",
    "                        middle_part = match.group(2)  # XMX part\n",
    "                        \n",
    "                        if a < b:\n",
    "                            direction = 1\n",
    "                        elif a > b:\n",
    "                            direction = -1\n",
    "                        else:  # a == b\n",
    "                            direction = 1  # or random choice as in conv file\n",
    "                        \n",
    "                        print(f\"  -> Start interface: {a}, End interface: {b}, Middle: {middle_part}, Direction: {direction}\")\n",
    "    \n",
    "    if not ptype_found:\n",
    "        print(\"No ptype patterns (aXMXb format) detected in the data\")\n",
    "        print(\"This data uses the standard infretis format\")\n",
    "    else:\n",
    "        print(f\"\\nPtype examples found: {list(set(ptype_examples))}\")\n",
    "    \n",
    "    print(f\"\\nData preview (first 3 rows):\")\n",
    "    for i in range(min(3, data_sample.shape[0])):\n",
    "        print(f\"Row {i}: {data_sample[i]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error analyzing data format: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "17516b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STEP 1: CALCULATING INFRETIS WEIGHTS ===\n",
      "Found ptype information in column 4\n",
      "Processed 9593 non-zero paths\n",
      "Interfaces: [-0.1, 0.0, 0.1, 0.2, 0.3]\n",
      "Path type information detected:\n",
      "  Forward paths (dir=1): 5937\n",
      "  Backward paths (dir=-1): 3656\n",
      "  Other paths (dir=0): 0\n",
      "\n",
      "Weight calculation summary:\n",
      "Number of interfaces: 5\n",
      "Interfaces: [-0.1, 0.0, 0.1, 0.2, 0.3]\n",
      "Number of paths processed: 9593\n",
      "\n",
      "Path statistics:\n",
      "Order parameter range: -0.099990 to 0.300710\n",
      "Paths crossing each interface:\n",
      "  λ_0 = -0.100000: 9593 paths\n",
      "  λ_1 = 0.000000: 8432 paths\n",
      "  λ_2 = 0.100000: 7523 paths\n",
      "  λ_3 = 0.200000: 6532 paths\n",
      "  λ_4 = 0.300000: 5598 paths\n",
      "\n",
      "=== STEP 2: COMPUTING WEIGHT MATRICES ===\n",
      "Computing weight matrices for 9593 paths\n",
      "Structure: Dictionary of 2D matrices [ensemble_idx][start_interface, end_interface]\n",
      "Dimensions: 5 ensembles x 5 interfaces x 5 interfaces\n",
      "Following original istar_analysis.py logic with tr=True\n",
      "Using ptype information with direction for istar_analysis-style computation\n",
      "Processed 9593 non-zero paths\n",
      "Interfaces: [-0.1, 0.0, 0.1, 0.2, 0.3]\n",
      "Path type information detected:\n",
      "  Forward paths (dir=1): 5937\n",
      "  Backward paths (dir=-1): 3656\n",
      "  Other paths (dir=0): 0\n",
      "\n",
      "Weight calculation summary:\n",
      "Number of interfaces: 5\n",
      "Interfaces: [-0.1, 0.0, 0.1, 0.2, 0.3]\n",
      "Number of paths processed: 9593\n",
      "\n",
      "Path statistics:\n",
      "Order parameter range: -0.099990 to 0.300710\n",
      "Paths crossing each interface:\n",
      "  λ_0 = -0.100000: 9593 paths\n",
      "  λ_1 = 0.000000: 8432 paths\n",
      "  λ_2 = 0.100000: 7523 paths\n",
      "  λ_3 = 0.200000: 6532 paths\n",
      "  λ_4 = 0.300000: 5598 paths\n",
      "\n",
      "=== STEP 2: COMPUTING WEIGHT MATRICES ===\n",
      "Computing weight matrices for 9593 paths\n",
      "Structure: Dictionary of 2D matrices [ensemble_idx][start_interface, end_interface]\n",
      "Dimensions: 5 ensembles x 5 interfaces x 5 interfaces\n",
      "Following original istar_analysis.py logic with tr=True\n",
      "Using ptype information with direction for istar_analysis-style computation\n",
      "Applying time-reversal symmetry (tr=True)\n",
      "  Applied tr edge case for ensemble 2: doubled weight_matrix_3d[2, 0, 1]\n",
      "  Applied tr edge case for last ensemble: doubled weight_matrix_3d[4, -1, -2]\n",
      "\n",
      "=== 3D WEIGHT MATRICES RESULTS (istar_analysis style) ===\n",
      "3D Matrix dimensions: 5 x 5 x 5\n",
      "Total weight processed: 130304.988095\n",
      "Total transitions: 30628.0\n",
      "Non-zero 3D matrix elements: 54\n",
      "Time-reversal symmetry applied: True\n",
      "\n",
      "Ensemble weight totals:\n",
      "  Sum weights ensemble 0: 0.0000\n",
      "  Sum weights ensemble 1: 30074.8929\n",
      "  Sum weights ensemble 2: 34049.9353\n",
      "  Sum weights ensemble 3: 32695.6782\n",
      "  Sum weights ensemble 4: 33484.4817\n",
      "\n",
      "2D Weight Matrix [start_interface, end_interface] (summed over ensembles):\n",
      "Rows = start interface, Columns = end interface\n",
      "Interface 0: 4867.0000 3223.4940 4805.0000 6164.5000 38607.0000 \n",
      "Interface 1: 3223.4940   0.0000 735.0000 748.5000 5468.0000 \n",
      "Interface 2: 4805.0000 735.0000   0.0000 434.0000 1625.5000 \n",
      "Interface 3: 6164.5000 748.5000 434.0000   0.0000 908.0000 \n",
      "Interface 4: 38607.0000 5468.0000 1625.5000 908.0000   0.0000 \n",
      "\n",
      "Transition Analysis:\n",
      "Forward transitions (j<k):  14646.500000 paths, 62718.9940 weight (48.1%)\n",
      "Backward transitions (j>k): 14646.500000 paths, 62718.9940 weight (48.1%)\n",
      "Self transitions (j=k):     1335.000000 paths, 4867.0000 weight (3.7%)\n",
      "\n",
      "Non-zero 3D matrix entries (first 10):\n",
      "  weights[1,0,0] = 4639.692857 (count: 1248.0)\n",
      "  weights[1,0,1] = 1012.505952 (count: 396.0)\n",
      "  weights[1,0,2] = 1516.223810 (count: 435.0)\n",
      "  weights[1,0,3] = 1406.862897 (count: 401.0)\n",
      "  weights[1,0,4] = 8782.007341 (count: 2422.5)\n",
      "  weights[1,1,0] = 1012.505952 (count: 396.0)\n",
      "  weights[1,2,0] = 1516.223810 (count: 435.0)\n",
      "  weights[1,3,0] = 1406.862897 (count: 401.0)\n",
      "  weights[1,4,0] = 8782.007341 (count: 2422.5)\n",
      "  weights[2,0,0] = 227.307143 (count: 87.0)\n",
      "Weight matrices computed successfully!\n",
      "Applying time-reversal symmetry (tr=True)\n",
      "  Applied tr edge case for ensemble 2: doubled weight_matrix_3d[2, 0, 1]\n",
      "  Applied tr edge case for last ensemble: doubled weight_matrix_3d[4, -1, -2]\n",
      "\n",
      "=== 3D WEIGHT MATRICES RESULTS (istar_analysis style) ===\n",
      "3D Matrix dimensions: 5 x 5 x 5\n",
      "Total weight processed: 130304.988095\n",
      "Total transitions: 30628.0\n",
      "Non-zero 3D matrix elements: 54\n",
      "Time-reversal symmetry applied: True\n",
      "\n",
      "Ensemble weight totals:\n",
      "  Sum weights ensemble 0: 0.0000\n",
      "  Sum weights ensemble 1: 30074.8929\n",
      "  Sum weights ensemble 2: 34049.9353\n",
      "  Sum weights ensemble 3: 32695.6782\n",
      "  Sum weights ensemble 4: 33484.4817\n",
      "\n",
      "2D Weight Matrix [start_interface, end_interface] (summed over ensembles):\n",
      "Rows = start interface, Columns = end interface\n",
      "Interface 0: 4867.0000 3223.4940 4805.0000 6164.5000 38607.0000 \n",
      "Interface 1: 3223.4940   0.0000 735.0000 748.5000 5468.0000 \n",
      "Interface 2: 4805.0000 735.0000   0.0000 434.0000 1625.5000 \n",
      "Interface 3: 6164.5000 748.5000 434.0000   0.0000 908.0000 \n",
      "Interface 4: 38607.0000 5468.0000 1625.5000 908.0000   0.0000 \n",
      "\n",
      "Transition Analysis:\n",
      "Forward transitions (j<k):  14646.500000 paths, 62718.9940 weight (48.1%)\n",
      "Backward transitions (j>k): 14646.500000 paths, 62718.9940 weight (48.1%)\n",
      "Self transitions (j=k):     1335.000000 paths, 4867.0000 weight (3.7%)\n",
      "\n",
      "Non-zero 3D matrix entries (first 10):\n",
      "  weights[1,0,0] = 4639.692857 (count: 1248.0)\n",
      "  weights[1,0,1] = 1012.505952 (count: 396.0)\n",
      "  weights[1,0,2] = 1516.223810 (count: 435.0)\n",
      "  weights[1,0,3] = 1406.862897 (count: 401.0)\n",
      "  weights[1,0,4] = 8782.007341 (count: 2422.5)\n",
      "  weights[1,1,0] = 1012.505952 (count: 396.0)\n",
      "  weights[1,2,0] = 1516.223810 (count: 435.0)\n",
      "  weights[1,3,0] = 1406.862897 (count: 401.0)\n",
      "  weights[1,4,0] = 8782.007341 (count: 2422.5)\n",
      "  weights[2,0,0] = 227.307143 (count: 87.0)\n",
      "Weight matrices computed successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_184690/231872884.py:165: RuntimeWarning: invalid value encountered in divide\n",
      "  w = D[\"path_f\"] / D[\"path_w\"]\n",
      "/tmp/ipykernel_184690/231872884.py:169: RuntimeWarning: invalid value encountered in divide\n",
      "  w = w / np.sum(w, axis=0) * np.sum(D[\"path_f\"], axis=0)\n"
     ]
    }
   ],
   "source": [
    "# Calculate infretis weights\n",
    "print(\"=== STEP 1: CALCULATING INFRETIS WEIGHTS ===\")\n",
    "weight_results = calculate_infretis_weights(DATA_FILE, TOML_FILE, nskip=NSKIP)\n",
    "\n",
    "\n",
    "# Display key results\n",
    "print(\"\\nWeight calculation summary:\")\n",
    "print(f\"Number of interfaces: {len(weight_results['interfaces'])}\")\n",
    "print(f\"Interfaces: {weight_results['interfaces']}\")\n",
    "print(f\"Number of paths processed: {len(weight_results['path_data']['pnr'])}\")\n",
    "# print(f\"Local crossing probabilities (WHAM): {weight_results['ploc_wham']}\")\n",
    "\n",
    "# Show some statistics about the weights\n",
    "# unbiased_weights = weight_results['unbiased_weights']\n",
    "# print(f\"\\nWeight statistics:\")\n",
    "# print(f\"Total weight: {np.sum(unbiased_weights):.6f}\")\n",
    "# print(f\"Average weight: {np.mean(unbiased_weights):.6f}\")\n",
    "# print(f\"Weight range: {np.min(unbiased_weights):.6f} to {np.max(unbiased_weights):.6f}\")\n",
    "\n",
    "# Show path statistics\n",
    "maxops = weight_results['path_data']['maxop'][:, 0]\n",
    "print(f\"\\nPath statistics:\")\n",
    "print(f\"Order parameter range: {np.min(maxops):.6f} to {np.max(maxops):.6f}\")\n",
    "print(f\"Paths crossing each interface:\")\n",
    "for i, intf in enumerate(weight_results['interfaces']):\n",
    "    n_crossing = np.sum(maxops >= intf)\n",
    "    print(f\"  λ_{i} = {intf:.6f}: {n_crossing} paths\")\n",
    "\n",
    "# Compute weight matrices\n",
    "print(\"\\n=== STEP 2: COMPUTING WEIGHT MATRICES ===\")\n",
    "weight_matrices_results = compute_weight_matrices_weights(weight_results, tr=True)\n",
    "w_path = weight_matrices_results['weight_matrix_3d']\n",
    "w_path_2d = weight_matrices_results['weight_matrix_2d']\n",
    "print(\"Weight matrices computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b735f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 3: ANALYZING WEIGHT MATRICES ===\n",
      "Warning: Zero count detected for q[4][3] = 0.0, counts = [0. 0.]\n",
      "\n",
      "Intermediate transition probabilities (q matrix):\n",
      "[[1.     0.7327 0.882  0.8703 0.8623]\n",
      " [1.     0.     1.     0.8443 0.8796]\n",
      " [0.8173 1.     0.     1.     0.7893]\n",
      " [0.8641 0.8972 1.     0.     1.    ]\n",
      " [0.8451 0.9372 0.9375 1.     0.    ]]\n",
      "\n",
      "Final transition probabilities (p matrix):\n",
      "[[0.2673 0.0864 0.0838 0.0774 0.485 ]\n",
      " [1.     0.     0.1557 0.1017 0.7426]\n",
      " [0.8173 0.1827 0.     0.2107 0.7893]\n",
      " [0.7752 0.122  0.1028 0.     1.    ]\n",
      " [0.7425 0.1361 0.0589 0.0625 0.    ]]\n",
      "\n",
      "Local crossing probabilities computed successfully\n",
      "iSTAR transition matrix constructed successfully!\n",
      "iSTAR calculation details:\n",
      "Intermediate states matrix Mp shape: (7, 7)\n",
      "Boundary transition matrix D shape: (7, 2)\n",
      "z1 (boundary conditions): [0 1]\n",
      "z2 (intermediate solutions): [0.51096877 0.78324741 0.82732071 1.         0.         0.14307129\n",
      " 0.18056915]\n",
      "y1 (boundary probabilities): [0.51096877 0.        ]\n",
      "Global crossing probability: 0.0\n",
      "Global crossing probability (iSTAR): [0.51096877]\n"
     ]
    }
   ],
   "source": [
    "# Analyze using tistools\n",
    "print(\"\\n=== STEP 3: ANALYZING WEIGHT MATRICES ===\")\n",
    "from tistools import get_transition_probs_weights, construct_M_istar, global_pcross_msm_star, ploc_memory\n",
    "\n",
    "# Construct transition matrix using iSTAR approach\n",
    "p, q = get_transition_probs_weights(weight_matrices_results['weight_matrix_3d'])\n",
    "M_istar = construct_M_istar(p, 2*len(weight_matrices_results['interfaces']), len(weight_matrices_results['interfaces']))\n",
    "print(\"iSTAR transition matrix constructed successfully!\")\n",
    "# Calculate global crossing probability using iSTAR\n",
    "z1, z2, y1, y2 = global_pcross_istar(M_istar, doprint=True)\n",
    "istar_global_pcross = y1[0] if y1.shape[0] > 1 else None\n",
    "print(f\"Global crossing probability (iSTAR): {istar_global_pcross}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561d335",
   "metadata": {},
   "source": [
    "## Step 2: Compute Transition Probabilities\n",
    "\n",
    "Next, we calculate the transition probabilities between interfaces using the infretis weights. This forms the basis for the iSTAR transition matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e7e159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate transition probabilities\n",
    "print(\"=== STEP 2: COMPUTING TRANSITION PROBABILITIES ===\")\n",
    "\n",
    "# Try both methods to compare\n",
    "print(\"\\nMethod 1: Standard iSTAR approach\")\n",
    "try:\n",
    "    P_istar = \n",
    "    print(\"✓ iSTAR transition probabilities calculated successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error in iSTAR method: {e}\")\n",
    "    P_istar = None\n",
    "\n",
    "print(\"\\nMethod 2: Simplified direct counting approach\")\n",
    "try:\n",
    "    P_simple = compute_simplified_transition_probabilities(weight_results)\n",
    "    print(\"✓ Simplified transition probabilities calculated successfully\")\n",
    "    print(\"\\nSimplified transition probability matrix:\")\n",
    "    print(\"Rows: starting interface, Columns: ending interface\")\n",
    "    for i in range(P_simple.shape[0]):\n",
    "        print(f\"Interface {i}: {P_simple[i, :]}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error in simplified method: {e}\")\n",
    "    P_simple = None\n",
    "\n",
    "# Use the method that worked\n",
    "if P_simple is not None:\n",
    "    P = P_simple\n",
    "    method_used = \"Simplified\"\n",
    "elif P_istar is not None:\n",
    "    P = P_istar  \n",
    "    method_used = \"iSTAR\"\n",
    "else:\n",
    "    print(\"✗ Both methods failed!\")\n",
    "    P = None\n",
    "\n",
    "if P is not None:\n",
    "    print(f\"\\nUsing {method_used} method for further analysis\")\n",
    "    print(f\"Transition matrix properties:\")\n",
    "    print(f\"- Shape: {P.shape}\")\n",
    "    print(f\"- Row sums: {[np.sum(P[i, :]) for i in range(P.shape[0])]}\")\n",
    "    print(f\"- Determinant: {np.linalg.det(P):.6f}\")\n",
    "    \n",
    "    # Check if matrix is properly stochastic\n",
    "    row_sums = np.sum(P, axis=1)\n",
    "    is_stochastic = np.allclose(row_sums, 1.0)\n",
    "    print(f\"- Is stochastic: {is_stochastic}\")\n",
    "    if not is_stochastic:\n",
    "        print(f\"  Row sums: {row_sums}\")\n",
    "else:\n",
    "    print(\"Cannot proceed without transition probabilities!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228cf5ab",
   "metadata": {},
   "source": [
    "## Step 3: Construct iSTAR Transition Matrix and Calculate Global Pcross\n",
    "\n",
    "Now we construct the full iSTAR transition matrix and calculate the global crossing probability using the Markov model approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6245249b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct iSTAR transition matrix and calculate global crossing probability\n",
    "print(\"=== STEP 3: iSTAR MATRIX CONSTRUCTION AND GLOBAL PCROSS ===\")\n",
    "\n",
    "if P is not None:\n",
    "    N = len(weight_results['interfaces'])\n",
    "    print(f\"Number of interfaces: {N}\")\n",
    "    \n",
    "    # Construct the iSTAR transition matrix\n",
    "    try:\n",
    "        M = construct_istar_transition_matrix(P, N)\n",
    "        print(\"✓ iSTAR transition matrix constructed successfully\")\n",
    "        \n",
    "        # Display matrix properties\n",
    "        print(f\"\\nMatrix M properties:\")\n",
    "        print(f\"- Shape: {M.shape}\")\n",
    "        print(f\"- Non-zero entries: {np.count_nonzero(M)}\")\n",
    "        print(f\"- Matrix preview (first 5x5):\")\n",
    "        print(M[:5, :5])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error constructing iSTAR matrix: {e}\")\n",
    "        M = None\n",
    "    \n",
    "    # Calculate global crossing probability\n",
    "    if M is not None:\n",
    "        try:\n",
    "            print(\"\\nCalculating global crossing probability...\")\n",
    "            z1, z2, y1, y2 = global_pcross_istar(M, doprint=True)\n",
    "            \n",
    "            if y1 is not None and y1.shape[0] > 1:\n",
    "                global_pcross = y1[1, 0]  # Probability to reach state -1 before state 0\n",
    "                print(f\"\\n✓ Global crossing probability calculated: {global_pcross:.8f}\")\n",
    "                \n",
    "                # Store results for comparison\n",
    "                istar_results = {\n",
    "                    'transition_matrix': M,\n",
    "                    'global_pcross': global_pcross,\n",
    "                    'solution_vectors': (z1, z2, y1, y2)\n",
    "                }\n",
    "            else:\n",
    "                print(\"✗ Could not extract global crossing probability\")\n",
    "                global_pcross = None\n",
    "                istar_results = None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error calculating global crossing probability: {e}\")\n",
    "            global_pcross = None\n",
    "            istar_results = None\n",
    "    else:\n",
    "        print(\"Cannot calculate global crossing probability without transition matrix\")\n",
    "        global_pcross = None\n",
    "        istar_results = None\n",
    "else:\n",
    "    print(\"Cannot proceed without transition probabilities!\")\n",
    "    global_pcross = None\n",
    "    istar_results = None\n",
    "\n",
    "# Summary\n",
    "if global_pcross is not None:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"ISTAR ANALYSIS SUMMARY\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Global Crossing Probability: {global_pcross:.8f}\")\n",
    "    print(f\"Method used: {method_used} transition probabilities\")\n",
    "    print(f\"Number of interfaces: {N}\")\n",
    "    print(f\"Interface positions: {weight_results['interfaces']}\")\n",
    "else:\n",
    "    print(\"\\n✗ iSTAR analysis failed - could not calculate global crossing probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e93b39",
   "metadata": {},
   "source": [
    "## Step 4: Calculate Binless Crossing Probability\n",
    "\n",
    "For comparison, we also calculate the binless crossing probability directly from the weighted path data. This provides a complementary view of the crossing behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c691ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate binless crossing probability\n",
    "print(\"=== STEP 4: CALCULATING BINLESS CROSSING PROBABILITY ===\")\n",
    "\n",
    "try:\n",
    "    x_binless, y_binless = calculate_binless_pcross(weight_results)\n",
    "    print(\"✓ Binless crossing probability calculated successfully\")\n",
    "    \n",
    "    # Show some key values\n",
    "    interfaces = weight_results['interfaces']\n",
    "    print(f\"\\nBinless crossing probability at interfaces:\")\n",
    "    for i, intf in enumerate(interfaces):\n",
    "        # Find closest point in binless curve\n",
    "        idx = np.argmin(np.abs(x_binless - intf))\n",
    "        pcross_at_intf = y_binless[idx]\n",
    "        print(f\"  λ_{i} = {intf:.6f}: P_cross ≈ {pcross_at_intf:.6f}\")\n",
    "    \n",
    "    binless_results = (x_binless, y_binless)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error calculating binless crossing probability: {e}\")\n",
    "    binless_results = None\n",
    "\n",
    "# Prepare results for comparison plotting\n",
    "if binless_results is not None:\n",
    "    plot_results = {\n",
    "        'binless': binless_results,\n",
    "        'interfaces': weight_results['interfaces']\n",
    "    }\n",
    "    \n",
    "    if global_pcross is not None:\n",
    "        plot_results['istar_global'] = global_pcross\n",
    "        \n",
    "        # Compare global iSTAR result with binless at final interface\n",
    "        final_intf_idx = np.argmin(np.abs(x_binless - interfaces[-1]))\n",
    "        binless_final = y_binless[final_intf_idx]\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"COMPARISON SUMMARY\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"iSTAR Global P_cross:     {global_pcross:.8f}\")\n",
    "        print(f\"Binless at final λ:       {binless_final:.8f}\")\n",
    "        print(f\"Ratio (iSTAR/Binless):    {global_pcross/binless_final:.6f}\")\n",
    "        print(f\"Absolute difference:      {abs(global_pcross - binless_final):.8f}\")\n",
    "    \n",
    "    print(\"\\n✓ Ready for visualization\")\n",
    "else:\n",
    "    print(\"✗ Cannot prepare comparison plots without binless results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60192dc6",
   "metadata": {},
   "source": [
    "## Step 5: Visualization and Analysis\n",
    "\n",
    "Finally, we visualize the results and compare the different methods for calculating the crossing probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c90507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization and final analysis\n",
    "print(\"=== STEP 5: VISUALIZATION AND ANALYSIS ===\")\n",
    "\n",
    "if 'plot_results' in locals() and plot_results:\n",
    "    # Plot the comparison\n",
    "    print(\"Creating comparison plot...\")\n",
    "    plot_pcross_comparison(plot_results, \n",
    "                          title=\"iSTAR vs Binless Crossing Probability (infretis weights)\")\n",
    "    \n",
    "    # Additional analysis plots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Transition matrix heatmap (if available)\n",
    "    if istar_results is not None and 'transition_matrix' in istar_results:\n",
    "        M = istar_results['transition_matrix']\n",
    "        im1 = axes[0, 0].imshow(M, cmap='Blues', aspect='auto')\n",
    "        axes[0, 0].set_title('iSTAR Transition Matrix M')\n",
    "        axes[0, 0].set_xlabel('To State')\n",
    "        axes[0, 0].set_ylabel('From State')\n",
    "        plt.colorbar(im1, ax=axes[0, 0])\n",
    "    else:\n",
    "        axes[0, 0].text(0.5, 0.5, 'Transition Matrix\\nNot Available', \n",
    "                       ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "        axes[0, 0].set_title('iSTAR Transition Matrix M')\n",
    "    \n",
    "    # Plot 2: Local crossing probabilities\n",
    "    interfaces = weight_results['interfaces']\n",
    "    ploc = weight_results['ploc_wham']\n",
    "    axes[0, 1].semilogy(range(len(interfaces)), ploc, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[0, 1].set_title('Local Crossing Probabilities (WHAM)')\n",
    "    axes[0, 1].set_xlabel('Interface Index')\n",
    "    axes[0, 1].set_ylabel('P_loc')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Weight distribution\n",
    "    weights = weight_results['unbiased_weights']\n",
    "    axes[1, 0].hist(np.log10(weights[weights > 0]), bins=20, alpha=0.7, color='green')\n",
    "    axes[1, 0].set_title('Distribution of Path Weights')\n",
    "    axes[1, 0].set_xlabel('log₁₀(Weight)')\n",
    "    axes[1, 0].set_ylabel('Count')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Path statistics\n",
    "    maxops = weight_results['path_data']['maxop'][:, 0]\n",
    "    axes[1, 1].scatter(maxops, weights, alpha=0.6, s=20)\n",
    "    axes[1, 1].set_title('Path Weights vs Maximum Order Parameter')\n",
    "    axes[1, 1].set_xlabel('Maximum Order Parameter')\n",
    "    axes[1, 1].set_ylabel('Weight')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    \n",
    "    # Add interface lines\n",
    "    for intf in interfaces:\n",
    "        axes[1, 1].axvline(intf, color='red', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"✓ Visualization complete\")\n",
    "    \n",
    "else:\n",
    "    print(\"✗ Cannot create plots - missing results data\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"FINAL WORKFLOW SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if global_pcross is not None:\n",
    "    print(f\"✓ iSTAR Analysis Successful\")\n",
    "    print(f\"  Global Crossing Probability: {global_pcross:.8e}\")\n",
    "    print(f\"  Method: iSTAR with {method_used} transition probabilities\")\n",
    "else:\n",
    "    print(f\"✗ iSTAR Analysis Failed\")\n",
    "\n",
    "if binless_results is not None:\n",
    "    print(f\"✓ Binless Analysis Successful\")\n",
    "    print(f\"  Points calculated: {len(binless_results[0])}\")\n",
    "    print(f\"  Order parameter range: {binless_results[0].min():.3f} to {binless_results[0].max():.3f}\")\n",
    "else:\n",
    "    print(f\"✗ Binless Analysis Failed\")\n",
    "\n",
    "print(f\"\\nKey Improvements over Old Workflow:\")\n",
    "print(f\"• Uses proper infretis weights instead of simple path counts\")\n",
    "print(f\"• Implements WHAM-based local crossing probabilities\")\n",
    "print(f\"• Provides both iSTAR and binless methods for comparison\")\n",
    "print(f\"• Includes comprehensive error handling and diagnostics\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9c43e5",
   "metadata": {},
   "source": [
    "## Key Improvements and Usage Notes\n",
    "\n",
    "### Major Changes from Old Workflow:\n",
    "\n",
    "1. **Infretis Weight Integration**: The biggest change is using proper infretis weights (`path_f / path_w`) instead of simple path counting. This provides statistically correct weighting of paths.\n",
    "\n",
    "2. **WHAM-based Local Probabilities**: The calculation of local crossing probabilities now uses the WHAM methodology to properly combine data from different ensembles.\n",
    "\n",
    "3. **Robust Error Handling**: The workflow includes comprehensive error checking and fallback methods.\n",
    "\n",
    "4. **Multiple Methods**: Provides both iSTAR Markov model approach and direct binless calculation for comparison.\n",
    "\n",
    "### Understanding the Results:\n",
    "\n",
    "- **Global Pcross (iSTAR)**: Single value representing the overall transition probability calculated using the Markov state model\n",
    "- **Binless Pcross**: Continuous curve showing crossing probability as a function of order parameter\n",
    "- **Local Probabilities**: WHAM-weighted probabilities for transitions between adjacent interfaces\n",
    "- **Transition Matrix**: Full Markov model capturing the interface-to-interface dynamics\n",
    "\n",
    "### Usage for Different Systems:\n",
    "\n",
    "To use this workflow for your own system:\n",
    "1. Update `DATA_DIR`, `TOML_FILE`, and `DATA_FILE` paths\n",
    "2. Adjust `NSKIP` if you want to exclude initial equilibration\n",
    "3. The interfaces are automatically read from the TOML file\n",
    "4. All weight calculations are handled automatically\n",
    "\n",
    "### Validation:\n",
    "\n",
    "The workflow validates results by:\n",
    "- Comparing iSTAR global result with binless calculation\n",
    "- Checking matrix properties (stochastic, determinant)\n",
    "- Monitoring weight distributions and statistics\n",
    "- Providing comprehensive diagnostic output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
